---
title: "Introduction to Statistics & Machine Learning"
subtitle: "GTIPI Summer School"
author: "Irene Schmidtmann (Irene.Schmidtmann@uni-mainz.de)</br>"
institute: "IMBEI - University Medical Center Mainz"
date: "2022/05/30"
output:
  xaringan::moon_reader:
    css: ["default", "default-fonts","css/FMstyles.css","css/animate.css"]
    lib_dir: libs
    nature:
      highlightStyle: github
      highlightLines: true
      countIncrementalSlides: false
      titleSlideClass: [center, middle]
---
```{r setup, include=FALSE}
options(htmltools.dir.version = FALSE)
knitr::opts_chunk$set(
  eval = TRUE,
  message = FALSE,
  echo = TRUE,
  warnings = TRUE,
  fig.align = "center"
)

fig.count <- 0
table.count <- 1
```

```{r setup2, include=FALSE}
# library(readr)
library("ggplot2")
library("GGally")
library("dplyr")
library("knitr")
library("kableExtra")

library("DESeq2")
library("airway")
library("ade4")
```

# Topics

* Curse of dimensionality - or a blessing in disguise?

* Multiple testing

* How to extract essential information from data: PCA

* How to obtain parsimonious statistical models

---
class: inverse, center, middle

# Multiple testing

---
# Statistical tests - one and many
## Classical statistical tests

* An example

* Why test hypotheses and what to do with the result?

  + Neyman Pearson paradigm

  + Fisher's approach

## Testing many hypotheses

* Problems encountered when testing more than one hypothesis

* Possible solutions

---
# Classical statistical tests
##  Example

* We know that a certain trait *A* is present in 20% of the general population.

* We study a disease *D* possibly related to *A*

* Question: Is *A* more frequent in patients than in the general population?

* Study to answer this question: obtain a sample of patients with disease *D* and determine how often trait *A* is present.

* Compare frequency in patient sample to reference value.

---
# Classical statistical tests
##  Example (continued)
```{r BinomialTest1, include=TRUE, echo = FALSE, results = "asis"}
set.seed(0xdada)

numPatients = 100
probA = 0.3
```
* Simulated data for `r numPatients` fictious patients with disease D
```{r BinomialTest2, include=TRUE, echo = FALSE, results = "asis", fig.height = 6, fig.width = 6, fig.cap = "bla bla"}
patientTraits = sample(c("A", "not A"), size = numPatients,
                   replace = TRUE, prob = c(probA, 1 - probA))

kable(table(patientTraits), format = 'html', caption = paste0("Table ", table.count, ": Frequency of trait"), align = "c") %>%
  kable_styling(bootstrap_options = "striped", full_width = T, position = "float_right")
table.count <- table.count + 1      
```

* Statistical model:
  + Binomial distribution with parameters
      - $n =$ number of patients (here: $n = 100$),
      - $p =$ probability of observing trait *A* 
  + Random variable $K$ describes number of observed occurences of *A*
      
* To answer research question, test hypothesis

  + $H_0: p = 0.2$ against
  + $H_1: p > 0.2$
  
* Use binomial test to compare observed frequency of trait *A* to reference value

---
# Classical statistical tests
##  Example (continued)
```{r BinomialTest3, include=TRUE, echo = FALSE}
alpha <- 0.05
k <- 0:numPatients
numA = sum(patientTraits == "A")

# determine probability density and probability of upper tail for Binomial distribution
binomDensity <- tibble(k = k, p = dbinom(k, size = numPatients, prob = 0.2), prob_gt = pbinom(k, size = numPatients, prob = 0.2, lower.tail = FALSE))

binomDensity <- mutate(binomDensity, reject = (prob_gt <= alpha))
k_0 <- min(binomDensity$k[binomDensity$reject])
```

.pull-left[
* Determine test statistic $T = K$

* Reject null hypothesis if $T \ge k_0$ with 
  + $k_0 =$ critical value, chosen such that $P(T \ge k_0) \le \alpha$ if $H_0$ is true.
  + $\alpha =$ probability of type I error, i. e. of erroneously rejecting null hypothesis

* Choose $\alpha = 0.05$ $\Rightarrow k_0 =$ `r k_0`

* Observed $k = 30 \gt k_0=$ `r k_0` $\Rightarrow$ Reject null hypothesis
]

.pull-rigth[
```{r BinomialTest4, include=TRUE, echo = FALSE, results = "asis", out.width = "45%", fig.cap = paste0("Fig. ", fig.count, ": Distribution of T given `H_0` with rejection region")}
# Plot probability distribution and mark rejection
ggplot(binomDensity) +
  geom_bar(aes(x = k, y = p, col = reject), stat = "identity") +
  scale_colour_manual(
    values = c(`TRUE` = "red", `FALSE` = "darkgrey")) +
  geom_vline(xintercept = numA, col = "blue") +
  theme(legend.position = "none")
fig.count <- fig.count + 1
```
]
---
# Exact Binomial test in R
## One-sided test
```{r BinomialTest5, include=TRUE, echo = TRUE}
binom.test(x = numA, n = numPatients, p = 0.2, 
           alternative = "greater")
```
* p-value: Probability of observing an event at least as extreme as the one actually observed, given the null hypothesis is true.

---
# Why test hypotheses and what to do with the result?
## Neyman Pearson paradigm "Hypothesis testing"
* We need to make a decision, e. g. approve new medication

* originally decision between (just) 2 distributions - simple alternative hypothesis

* in practice usually composite alternative hypothesis

  + set up two statistical hypotheses null hypothesis $H_0$ and alternative hypothesis $H_1$
  
  + decide on $\alpha$ (probability of type I error) and $\beta$ (probability of type II error), necessary sample size follows
  
  + if data in rejection region of $H_0$, accept $H_1$
  
  + otherwise keep $H_0$

---
# Why test hypotheses and what to do with the result?
## Fisher's approach "Significance test"
* We want to learn from our data

  + require null hypothesis
  
  + report exact level of significance (p-value)
  
* p-value as measure for credibility of null hypothesis

##  Overall
* Unresolved dispute over formulations     
---

```{r Figure_xkcd_significance, echo=FALSE, eval=TRUE, out.width = "28%", fig.pos="h", fig.cap = paste0("Fig.", fig.count, ": https://imgs.xkcd.com/comics/significant.png"), fig.align = "center"}
include_graphics(path = "https://imgs.xkcd.com/comics/significant.png")
fig.count <- fig.count + 1
```


---
# Testing many hypotheses
## Problems encountered when testing more than one hypothesis

* Increase in type I error 

* Temptations
  + ... looking for other tests if the first one did not give the desired result

  + ... changing the  hypothesis if the original one did not give the desired result
  
  + ... and even worse: HARKing â€“ hypothesizing after the results are known

* Such proceeding violates the rules and underlying assumptions of hypothesis testing.


---
# Testing more than one hypothesis
Table `r table.count`: Types of error in multiple testing. 
`r table.count <- table.count + 1`

|Test vs reality | $H_0$ is true | $H_0$ is false | Total   |
|: --------------|---------------|----------------|---------|
| Rejected       | $V$           | $S$            | $R$     |
| Not rejected   | $U$           | $T$            | $m - R$ |
| Total          | $m_0$         | $m - m_0$      | $m$     |

with
* $m$: total number of hypotheses
* $m_0$: number of true null hypotheses
* $V$: number of false positives (a measure of type I error)
* $T$: number of false negatives (a measure of type II error)
* $S, U$: number of true positives and true negatives
* $R$: number of rejections

$m$ is known and $R$ can be observed.

---
# Family wise error rate
* Family wise error rate (FWER): is the probability that V>0, i.e. that we 
reject at least one true null hypothesis, i. e. we produce one or more false 
positive results.

* If all tests are independent we find

$$P(V>0) = 1 - P( \mbox{no rejection of any of } m_{0} \mbox{ nulls} ) \\ 
         = 1 - (1-\alpha)^{m_{0}} \rightarrow 1 \mbox{ as } m_{0} \rightarrow \infty$$
         
* Worst case:  $P(V>0) = \min(1, m_{0} \cdot \alpha)$

* Serious problem if thousands of genes are to be tested

---
# Controlling family wise error rate
## Bonferroni correction

* $m_0 < m \Rightarrow$ we are on the safe side (in terms of type I error) if we choose $\alpha = \alpha_{\tiny{\mbox{FWER}}}/m$

* Drawback: Required individual $\alpha$'s become extremely small

    $\Rightarrow$ $\beta$'s increase, i. e. loss of power even for moderate 
    number of tested hypotheses $m$  

---
# A different concept of error control: false discovery rate (FDR)
* No longer try to keep the FWER below $\alpha$', but require a boundary on the
proportion of erroneously rejected null hypothese

* FDR is the **expected** proportion of type I errors out of the rejections made

* $\mathrm{FDR} = \mathrm{E}\left[\frac{V}{\max (R, 1)}\right] = \mathrm{E}\left[\frac{V}{R} \mid R > 0 \right] \cdot P(R > 0)$

* FDR = FWER if **all** null hypotheses are true

* As FDR is an expectation (average), individual FDR could be much worse

---
# How to control false discovery rate
## Example
(Taken from Holmes, Huber. Modern statistics for modern biology 6.9)
* Use RNA-Seq dataset airway

  + contains gene expression measurements (gene-level counts) of four primary human airway smooth 
  muscle cell lines with and without treatment with dexamethasone
  
  + interest is in differential expression
  
  + use the DESeq2 method (more later, see alsoe chapter 8 in Modern statistics for modern biology)
  
* Obtain large number of p-values

* How to decide which hypotheses should be rejected?


---
# Controlling false discovery rate
## p-value histogram
* p-value histogram exhibits mixture composed of two components:

  + p-values resulting from the tests for which the null hypothesis is true.

  + p-values resulting from the tests for which the null hypothesis is false.

* relative size of these two components depends on the fraction of true nulls and true alternatives

  + can often be visually estimated from the histogram
  
      - peak near 0 for false null hypotheses
      
      - uniform distribution for true null hypotheses

---
# Controlling false discovery rate
## p-value histogram
```{r pvalueHistogram1, include=TRUE, echo = FALSE, results = "asis", out.width = "50%", fig.cap = paste0("Fig. ", fig.count, ": p-value histogram for airway data")}
fig.count <- fig.count + 1
# Load airway data and perform DESeq
data("airway")
aw   = DESeqDataSet(se = airway, design = ~ cell + dex)
aw   = DESeq(aw)
awde = as.data.frame(results(aw)) %>% dplyr::filter(!is.na(pvalue))

ggplot(awde, aes(x = pvalue)) +
  geom_histogram(binwidth = 0.025, boundary = 0)
```

---
# Controlling false discovery rate
## p-value histogram
```{r pvalueHistogram2, include=TRUE, echo = FALSE, results = "asis", out.width = "50%"}

alpha = binw = 0.025
# Estimate pi_0
pi0 <- 2 * mean(awde$pvalue > 0.5)
```
.pull-left[
* $\alpha = 0.025$ 
* Given the observed distribution of p-values, 
`r round(pi0 * binw * nrow(awde))` of the `r nrow(awde)` p-values are likely to 
correspond to true null hypotheses
* There are `r sum(awde$pvalue <= alpha)` p-values $< \alpha$
* Estimated proportion of false rejections: `r round(pi0 * alpha / mean(awde$pvalue <= alpha), 4)`
]

.pull-right[
```{r pvalueHistogram3, include=TRUE, echo = FALSE, results = "asis", out.width = "100%", fig.cap = paste0("Fig. ", fig.count, ": Visual estimation of the FDR with the p-value histogram.")}
fig.count <- fig.count + 1
ggplot(awde,
  aes(x = pvalue)) + geom_histogram(binwidth = binw, boundary = 0) +
  geom_hline(yintercept = pi0 * binw * nrow(awde), col = "blue") +
  geom_vline(xintercept = alpha, col = "red")

```
]
---
# Controlling family false discovery rate
## Benjamini-Hochberg algorithm

* First, order the p-values in increasing order, $p_{(1) \cdots} p_{(m)}$

* Then for some choice of $\varphi$ ( target FDR), find the largest value of k that
satisfies: $p_{(k)} < \varphi k / m$

* Finally reject the hypotheses $1, \ldots, k$ corresponding to $p_{(1) \cdots} p_{(k)}$

---
# Controlling false discovery rate
## Benjamini-Hochberg algorithm
```{r BenjaminiHochberg1, include=TRUE, echo = FALSE, results = "asis"}
fig.count <- fig.count + 1

phi  <- 0.05
awde <- mutate(awde, rank = rank(pvalue))
m    <- nrow(awde)

kmax  <-  with(arrange(awde, rank),
          last(which(pvalue <= phi * rank / m)))
```
.pull-left[
* Choose e. g. $\varphi =$ `r phi`
* Number of rejected null hypotheses: `r kmax`
* Boundary for p-value: `r round(phi * kmax/m, 4)`
* For comparison Bonferroni boundary for $\alpha =$ 0.05: `r formatC(0.05/nrow(awde), digits=8, format="f")`
]

.pull-right[
```{r BenjaminiHochberg2, include=TRUE, echo = FALSE, results = "asis", out.width = "100%", fig.cap = paste0("Fig. ", fig.count, ": Visualization of the Benjamini-Hochberg procedure.")}

ggplot(dplyr::filter(awde, rank <= 5000), aes(x = rank, y = pvalue)) +
  geom_line() + geom_abline(slope = phi / m, col = "red")
```
]

---
# Some extensions
* Take dependence between hypotheses into account

* Weigh p-values

* Sophisticated procedures for moderate number of hypotheses in clinical trials
  
  + gate-keeping
  
  + repeated significance testing during study 
  
      - interim analyes
      - group sequential trials
 
---
class: inverse, center, middle

# Linear model selection and regularization

---
# Linear regression
* Linear regression is commonly used to describe the relationship between

  + quantitative response $Y$ and
  
  + a set of variables $X_1, X_2, \ldots, X_p$
  
$Y = \beta_0 + \beta_1 \cdot X_1 + \beta_2 \cdot X_2 + \cdots + \beta_p \cdot X_p + \varepsilon$ 

where 
$\varepsilon$ is usually assumed to be a normally distributed error: $\varepsilon \sim \mathcal{N}(0, 1)$

---
# References
* Susan Holmes, Wolfgang Huber. Modern Statistics for Modern Biology. Cambridge University Press, 2019. The book is also online: https://www.huber.embl.de/msmb/

* Gareth James, Daniela Witten, Trevor Hastie, Robert Tibshirani. An Introduction to Statistical Learning. Second edition. Springer New York, 2021. PDF (legally) available at https://hastie.su.domains/ISLR2/ISLRv2_website.pdf

* Trevor Hastie, Robert Tibshirani. The Elements of Statistical Learning. pringer New York, 2009 PDF (legally) available at https://web.stanford.edu/~hastie/ElemStatLearn/printings/ESLII_print12_toc.pdf

* Frank E. Harrell. Regression Modeling Strategies. Second edition. Springer Cham 2015.

* https://en.wikipedia.org/wiki/Statistical_hypothesis_testing (especially for a comparison between Fisherian, frequentist (Neymanâ€“Pearson) interpretation)

* https://xkcd.com/882/
